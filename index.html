<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AI Ethics Blog</title>
     
    <link rel="stylesheet" href="styles.css">
</head>
  <body>

    <!-- particles.js container --> <div id="particles-js" style="z-index: -1; position: fixed;"></div>  <!-- particles.js lib - https://github.com/VincentGarreau/particles.js --> <script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script> <!-- stats.js lib --> <script src="https://threejs.org/examples/js/libs/stats.min.js"></script>
    <header>
        <p class="header-text">Giancarlo Massimo - Intro to AI Ethics Class Blog - PHL 304T</p>
    </header>



    <div class="container">
        <h1>Blog Post 10</h1>
        <h3>November 10, 2023</h3>
        
        <hr>

        <p>
            This week we read Elizabeth Anderson’s, “What is the Point of Equality?” which argues for a fundamental shift in the way we view justice and the goals of egalitarianism. Overall, I think Anderson’s critiques are extremely plausible and important if we want to define a truly just society. 
        </p>

        <p>
            We can look at the idea of luck egalitarianism and how such a conception of equality would influence policy on AI. I’ll argue that applying luck egalitarianism to AI produces similar problems as described by Anderson, and that taking her critiques into account can help us produce better policy in response to the rapid development of AI. 
        </p>

        <p>
            One of the most discussed issues with AI is the fear that it will replace the jobs of a significant amount of the population. In response to this, we may want to invoke luck egalitarianism. We could take into account the fact that workers being replaced by AI did not voluntarily accept any risk in being replaced by AI. In other words, they were simply unlucky. So, we may want to create forms of aid (perhaps some form of material compensation) to workers who have lost their jobs as a result of AI.
        </p>

        <p>
            On the other hand, Anderson’s critique of this form of egalitarianism is that it inherently requires a lack of respect towards those in need of help. It would create a simple transfer of wealth between those who are fortunate to still have jobs and those who lost their jobs, rather than focusing on the fundamental causes of oppression.
        </p>

        <p>
            Anderson’s conception of equality demands that we treat all people with inherent dignity, focusing on oppression and exploitation rather than “bad luck” which implies natural inferiority. I think this is important if we want to get to the actual root of injustice. We should instead focus on the inherent exploitation that results from the unequal ownership and control over the production of AI. 
        </p>

        <h1>Blog Post 9</h1>
        <h3>November 3, 2023</h3>
        
        <hr>

        <p>
            This week in class we covered social contract theory and Rawls’ theory of justice. Rawls established two key principles of justice that outline conditions that must be met for us to have achieved justice. According to the Stanford Encyclopedia of Philosophy, these are: 
        </p>

        <p>
            First Principle: Each person has the same indefeasible claim to a fully adequate scheme of equal basic liberties, which scheme is compatible with the same scheme of liberties for all.
        </p>

        <p>
            Second Principle: Social and economic inequalities are to satisfy two conditions:
        </p>

        <p>
            (a) They are to be attached to offices and positions open to all under conditions of fair equality of opportunity.
        </p>

        <p>
            (b) They are to be to the greatest benefit of the least-advantaged members of society (the difference principle)
        </p>

        <p>
            We spent a significant amount of time focusing on the first principle in class. Specifically, trying to unpack what it means and thinking about why Rawls decided to establish that principle as the most important principle. While I find it plausible that the first principle is in fact the most important and that the second principle follows from and exists to protect the first, I found the second principle to also be very interesting, and I wanted to spend some time on that in this post. 
        </p>

        <p>
            Particularly, I find part (b) of the second principle to be very interesting. Rather than trying to avoid social inequalities altogether, Rawls is arguing that any inequalities that must exist should be to the benefit of everyone. This raises the question: what *are* inequalities that satisfy this condition? What are some examples of social and economic inequalities that we might say are actually to the benefit of the most disadvantaged members of society?
        </p>

        <p>
            One example I can think of ties into our discussion of democracy from last week. In a truly democratic society, everyone votes for every decision the government makes, and every vote is decided by simple majority. However, it is generally accepted that this form of democracy is impractical and inefficient, so we instead elect representatives to legislate on our behalf. This does in fact create a form of social inequality between lawmakers and non-lawmakers, but we generally all agree that, fundamentally, this is to the benefit of everyone. Thus, as long as condition (a) is also satisfied regarding government positions, then this form of social inequality is justified under a Rawlsian perspective. I think the fact that Rawls can account for these types of situations is a strong advantage of his theory.  
        </p>

        <h1>Blog Post 8</h1>
        <h3>October 27, 2023</h3>
        
        <hr>

        <p>
            Continuing previous week’s discussion of the broader politics surrounding AI, this week we discussed democracy specifically. First, we clarified what exactly we mean when we talk about democracy, and then we came up with some ideas for how AI and algorithms more generally can affect the democratic process.
        </p>

        <p>
            We agreed that in general, a democratic system needs to have the following characteristics. 1) The government must be run by majority rule. 2) There must be an accessible way for the voting population to genuinely influence how the government is run. 3) For this system to function properly, there must be a free flow of information. 4) The right to vote must be universal (with perhaps minimal exceptions). There are a few more, but these were the ones I felt were most important. 
        </p>

        <p>
            Already, we can see how algorithms can both help and harm this process. For this post, I’ll focus on the harms. Gerrymandering is a clear problem, along with algorithms that we don’t understand controlling the information we receive. It’s also now possible to create fake videos and fake voices of politicians that may influence voters.  There are also some arguable subtler dangers too. When we use a voting machine, we want to be sure that the vote we cast is the vote that gets counted. 
        </p>

        <p>
            All of these dangers have a common theme of opaqueness and lack of trust. So are these intrinsic qualities of AI or are they a byproduct of the way AI fits into our society’s existing power structures? In 2019, Stephen Cave published an article titled “To save us from a Kafkaesque future, we must democratise AI.” I think Cave does a good job in this article outlining how AI has historically been controlled by those with the economic and political power to shape AI in such a way to reinforce that power. The headline suggests that we may be able to apply the characteristics of democracy that I outlined above to fundamentally change the role that AI plays in society. 
        </p>

        <p>
            However, Cave spends more time criticizing the current state of AI and does not go into detail here about how such a vision might be achieved. Admittedly it is a very difficult problem to solve, but I noticed that this sentiment appears to align with the “free software movement” which has existed since the 1980s.  [https://www.gnu.org/philosophy/free-software-intro.en.html, https://www.gnu.org/philosophy/free-sw.html]. I remember learning about this movement a long time ago, and I’m interested to see how well these ideas might hold up today, how they might relate to Cave’s point, and how they could be expanded on.
        </p>


        <h1>Blog Post 7</h1>
        <h3>October 20, 2023</h3>
        
        <hr>

        <p>
            This week we discussed the idea of “human progress.” More specifically, to say that we have made genuine progress, we must first define exactly what it is that we’re striving for. I think as a baseline statement, the fact that humanity has made progress is hard to dispute, if anything for the fact that if we were to ask people, “would you rather live right now or 10,000 years ago?” nearly everyone would prefer to live right now.
        </p>

        <p>
            However, I think there is good reason to believe that we frequently overestimate the progress humanity has made to genuinely improve the way that we allocate the resources available to us. 
        </p>

        <p>
            For a lot of us, there is a clear connection between technological progress and human progress. At first this makes sense: if we have a problem, we build something to solve it. This sentiment can be seen everywhere online today (which Ross Andersen calls the “Age of Foomscrolling”)—with all our problems, we hold out hope that the next technological innovation will produce a huge leap in quality of life and free us from our limitations. 
        </p>

        <p>
            This pattern of thought is so pervasive in our culture that we view it as the natural state of the world, not as an ideological position that can be examined, defended and argued like any other. The issue of course is that with all our technology, there always seems to be new problems that rise to take the place of what is solved. 
        </p>

        <p>
            This week we looked at how Marx can help explain this phenomenon. I find his materialist analysis to be very plausible because he makes an explicit point to build off of empirical observations. Under his view, the ideology of “techno-optimism” comes from the ruling class—in our case, Silicon Valley CEOs in control of the means to produce artificial intelligence. As a result, we shouldn’t view techno-optimism as the default way the world works. Genuine calls for political reform in the face of rapidly developing A.I. are far too often dismissed as too “ideologically charged.” After all, why can’t we just “leave politics out of it?” But that statement is just as politically charged as any other. If we choose to ignore alternative possibilities, such as for example pushing for the democratization of A.I., then we are in fact simply affirming the position of the ruling class, not being neutral or pragmatic.
        </p>


        <h1>Week 6 Blog Post</h1>
        <h3>September 29, 2023</h3>
        
        <hr>

        <p>
            This week we focused on the concept of “fairness” and how it relates to artificial intelligence and algorithms more generally. Though most of us share an intuitive understanding of what is fair, it can be difficult to articulate, much less formalize into actionable steps that can be carried out by an algorithm. 
        </p>

        <p>
            To make algorithms fair, we first have to determine what constitutes discrimination, which can be surprisingly difficult. For example, we would all agree that certain forms of discrimination are deeply wrong when they involve characteristics like race, gender, religious affiliation, etc. However, we might also think that there are legitimate grounds for discrimination on certain characteristics—should an algorithm be allowed to use the fact that someone is a smoker/non-smoker to determine whether to hire someone for a job?
        </p>

        <p>
            Once we have identified unjust forms of discrimination we’d like to avoid, there’s also the issue of how to deal with them. Our first thought might be to simply not allow algorithms to take certain characteristics like race and gender as an input. After all, if the algorithm has no idea about these characteris, how could it possibly discriminate based on them?
        </p>

        <p>
            There are clear issues with this position, however. In one of the papers we covered in class, we looked at the notion of deontic justice, which is less concerned with the fact that there is inequality, and is more concerned with the historical and social factors that caused these inequalities to arise in the first place. Using this account of justice, we have grounds to explicitly take these factors into account when designing algorithms. In an unjust society, a blind algorithm will simply perpetuate (and even amplify through positive/negative feedback loops) existing discrimination and disguise it as objective fairness. When we offload our decision making into the hands of algorithms, we wrongly give ourselves a way of deflecting responsibility for the decisions they make.
        </p>

        <p>
            In class we used as an example algorithms that predict a person’s chance to reoffend after being convicted of a crime, and Chapter 8 of Weapons of Math Destruction introduces the example of “e-scores” that can affect how companies like Capital One draw conclusions about us. Cathy O’Neil writes: “[I] ask them whether it makes sense to use ‘race’ as an input in the model. They inevitably respond that such a question would be unfair and probably illegal. The next question is whether to use ‘zip-code.’ This seems fair enough, at first. But it doesn’t take long for the students to see that they are codifying past injustices into their model.” A model that doesn’t actively correct for existing injustices simply reinforces them. An algorithm that is trained on enough data will be able to predict several characteristics about us indirectly, likely leading to unjust discrimination. The exact measures that can be taken to correct these algorithms is hard to quantify, though it is clear that the issue deserves attention.
        </p>


        <h1>Week 5 Blog Post</h1>
        <h3>September 22, 2023</h3>
        
        <hr>

        <p>
            Because algorithms have a strong tendency to become self-fulfilling prophecies, they have a profound potential to manipulate people’s perspectives on the world. This inevitably grants huge amounts of power to those who develop and understand these algorithms, and causes several ethical issues to arise concerning how that power should be allowed to be used. As a result, the government should directly intervene in regulating A.I., and it should enforce transparency in how machine learning models are trained. 
        </p>

        <p>
            One example of algorithms becoming self-fulfilling prophecies is given by Weapons of Math Destruction. In chapter 3, the book describes how the US News rankings of colleges can lead to a self-reinforcing cycle, where schools that are initially ranked higher tend to continue to be ranked higher, and lower rank schools tend to continue to be ranked lower. The details of how the ranking is made is not exactly transparent, hidden behind a statistical model that only a few understand, and yet it has manipulated the way we view college excellence. 
        </p>

        <p>
            On top of that, statistical models risk reducing abstract notions (such as excellence) to a few arbitrary measurements, which in this case would encourage colleges to maximize these metrics instead of excellence itself. As a result, these models can essentially hijack the value systems of these educational institutions, shifting their values from excellence by itself to whatever metrics are defined by the statistical model, which may or may not give a complete picture. 
        </p>

        <p>
            Evaluating this situation from a Kantian perspective, it is clear that the case described in chapter 3 of Weapons of Math destruction is a violation of the categorical imperative, and it gives us good reason to argue that the government should enforce greater transparency in A.I. overall. For example, these ranking algorithms incentivize colleges to treat their students as a mere means to increase those same rankings, essentially as “investments.” Additionally, since these value systems can be manipulated to make colleges prioritize rankings over excellence itself, and the supposed goal of the rankings is to show which colleges are more excellent than others, we have a contradiction in conception by the first formulation of the categorical imperative. 
        </p>


        <h1>Week 4 Blog Post</h1>
        <h3>September 15, 2023</h3>
        
        <hr>

        <p>
            This week we expanded our discussion of ethics to not only include the rightness of an action, but also notions of responsibility, which is crucial if we want to apply our ethical theories to either praise or blame someone. Determining moral responsibility for an action is a difficult problem, because there are a number of circumstances that would seem to absolve a person of responsibility, which we call excuses.
        </p>

        <p>
            There are two distinct kinds of excuses. For example, if I bump into someone while walking, then I could apologize and say that I didn’t see them. On the other hand, I could apologize and explain that someone else pushed me into them. Both of these excuses seem better than intending to bump into someone. The first example, however, could be called negligence, whereas I could not be called negligent for my actions in the second example, because I literally could not have done otherwise if I was pushed. 
        </p>

        <p>
            So, a central question we have to ask is whether examples of “cultural ignorance” are examples of the first kind or the second kind. On one hand, we could argue that a person whose society culturally pressures them into acting wrongly is literally incapable of questioning that action—they simply have no realistic way of knowing otherwise. However, we can also point out examples of “affected ignorance” that is essentially self imposed—we could know the consequences of our actions, but we choose not to for the sake of our own comfort. 
        </p>

        <p>
            To gain some insight, we could apply this question to the case of Tesla Autopilot, which has been involved with 17 fatalities and 736 crashes since this June. Is Musk simply acting on the widely held belief that we should pursue technological progress at nearly any cost, or is there deeper negligence at play? Musk claims the technology is far safer than driving normally, but the rapid pace at which it has been deployed, the way it’s been misrepresented in its marketing, and the way that key technical components have been removed, suggest that Musk is choosing to ignore the potential harm of his actions instead of purely focusing on safety. 
        </p>

        <p>
            Additionally, while Musk certainly directs the company to commercialize and promote itself rapidly, regardless of the current state of technology, he is certainly not the only person involved in making it. There are over 300 engineers that currently work on Autopilot, so even if we grant that the technology is safer than normal driving, it’s difficult to attribute that success to Musk. 
        </p>



        <h1>Week 3 Blog Post</h1>
        <h3>September 8, 2023</h3>
        
        <hr>

        <p>
            This week we covered Kantian ethics, which seems to be a huge contrast to other ethical theories. I think that there’s a lot to like about Kantian ethics, particularly that it essentially eliminates problems involving “moral luck,” which I saw as a significant problem with Utilitarianism. I also appreciate the fact that desire has a secondary role to reason, because people’s desires change all the time from context to context. If instead we are able to reason morally, that gives us a better ability to override whatever our current desires are and do the right thing. 
        </p>

        <p>
            However, I think the article “What Can You Do When A.I. Lies About You?” highlights some interesting aspects of the way Kant characterizes moral agents. What we get with AI is something that seems to have human-like characteristics but without any actual ability to reason morally.
        </p>

        <p>
            We typically don’t put moral blame on technology when it fails. We don’t think a car or an airplane is immoral if it malfunctions. We might blame the people who make the car or the plane if they were careless and especially if they put other people in danger, and to a degree that’s true for AI as well. But I think there’s an important distinction to be made with a technology that has no sentience of its own, is a purely statistical language model, yet has the appearance of human communication, to which it seems we should be able to apply moral standards. 
        </p>

        <p>
            I don’t think the technology shouldn’t exist, (and even if that was what we wanted, it’s already here), but I do see a problem with tech companies marketing them as good sources of information.
        </p>

        <p>
            It’s definitely a mistake to view a statistical model with the appearance of a human as something that can have respect for human values. Kant says that all rational beings should be capable of recognizing the categorical imperative as something fundamentally good, that the autonomy of other sentient beings is something valuable in itself, and it follows that rational beings ought not lie. We might be at risk of assuming the same is true of something that very convincingly replicates human language, so when it tells us something, we might be at risk of believing it uncritically. 
        </p>

        <p>
            AI cannot possibly in its current form understand the importance of fact-checking itself to avoid accidentally lying about someone else, no matter how intelligent it appears. Developers specifically need to make it tell the truth as much as possible (which is not a straightforward problem to solve), and it should be well understood by the general public that “truth” is not an value inherent to AI. 
        </p>

        <h1>Week 2 Blog Post</h1>
        <h3>September 1, 2023</h3>
        
        <hr>

        <p>
            This week we were asked to respond to "I Shouldn’t Have to Accept Being in Deepfake Porn" by Nina Jankowicz in The Atlantic with either a utilitarian or virtue ethicist evaluation. I’ll be giving a utilitarian evaluation of the article because I think it highlights the ways that oversimplified formulations of utilitarianism can lead to obviously terrible conclusions, and the ways that we might look to revise utilitarianism while keeping its core principles. 
        </p>

        <p>
            This case is not ethically challenging in the sense that we wonder what the conclusion should be—no remotely moral person could ever justify the creation of deepfake porn. Rather, it’s challenging in the sense that it makes us rethink our general moral principles and adjust them to be more robust. This is an especially important goal considering that with the rapid pace of technological advancement, new ethical challenges are always arising. 
        </p>

        <p>
            We can start with a basic form of utilitarianism: an action is considered ethical if it increases pleasure and minimizes pain for the greatest number of people. While this principle seems intuitive at first, we can quickly see how it leads to extremely terrible conclusions. In the case of deepfake porn, Jankowicz writes: “By simply existing as women in public life, we have all become targets, stripped of our accomplishments, our intellect, and our activism and reduced to sex objects for the pleasure of millions of anonymous eyes.” The most basic form of utilitarianism would seem to suggest that the pleasure of millions outweighs that of one, even if it means that one person’s fundamental rights are atrociously disregarded. 
        </p>

        <p>
           This case may give us reason to give up utilitarianism entirely and adopt more of an “Ethics of Care” approach. Perhaps there are no general principles we can universally appeal to, and each case needs to be evaluated in the context of empathy. Deepfake porn is categorically wrong simply because we have empathy for each other as human beings.  
        </p>

        <p>
            However, I do think there are ways that the utilitarian view can adapt. I’ll bring in an example we went over in class. Suppose we are all stranded on an island. We collectively decide to be a utilitarian society and to divide up the island into plots of land for each of us. Now suppose that we want to build a swimming pool in the middle of the island, taking away someone’s plot of land. The result is a loss of pleasure for the person whose plot was taken, but an increase in pleasure for everyone else, so basic act-utilitarianism would counterintuitively conclude that this is morally acceptable. 
        </p>

        <p>
            Nevertheless, we can introduce rule-utilitarianism, and say that we should increase overall utility, but not at the expense of anyone’s fundamental rights. In this case, we would conclude that we cannot build a swimming pool. On the other hand, we can modify the example and instead say that there is an important medicine buried underneath one person’s plot of land, and we need it to save everyone else on the island. In this case, the need to save everyone would seem to outweigh the rule that we shouldn’t take anyone’s plot of land, so rule-utilitarians are still willing to break the rules if necessary. The question then becomes: when is it okay for us to break the rules? It seems that the case of the swimming pool and medicine are identical, except at different extremes—in both cases we are trading a decrease in utility for one person for an increase in utility of many others. Where should the line be?
        </p>

        <p>
            This is where I believe the rule-utilitarian can respond and say that the two cases are not the same at all. It’s true that we trade a decrease in one person’s utility for an increase in the utility of many others, but there ought to be a morally significant difference between increasing pleasure and alleviating suffering. In the swimming pool example, everyone may be doing completely fine and would simply enjoy having a pool, but in the medicine example there is risk of great suffering if we don’t break the rule. 
        </p>

        <p>
            Finally, we can see how this applies to the case of deepfake porn. If we agree that people ought to have fundamental rights to bodily autonomy, privacy, consent, etc., then under a rule-utilitarian view, it is never acceptable to violate these rights, no matter how many people may wrongfully enjoy it. Only at the risk of many people suffering greatly could we consider breaking the rules, and there is obviously no such risk here. We can even bring in the views of John Sturart Mill and argue that on top of that, there are higher and lower pleasures, the lower pleasures being the ones we share with animals. So even if people do gain pleasure, those pleasures are not worth much to begin with, certainly not enough to commit an egregious violation of a person’s basic rights. 
        </p>

        <h1>Introduction to AI Ethics</h1>
        <h3>August 25, 2023</h3>
        
        <hr>
        <h2 class="section-header">What interests you?</h2>

        <p>
            Though I'm a computer science major, I developed a deep interest in philosophy throughout highschool. I liked the process of unpacking layers of complexity in everyday things, and I knew, that whether conscious of it or not, my view of the world is influenced by countless hidden assumptions. 
        </p>

        <p>
            I saw philosophy as a way to challenge those assumptions. It was immediately obvious that it's basically impossible to reach any satisfying conclusions, but maybe over time I could learn to ask better questions and even gain some small measure of control over them. 
        </p>

        <p>
            I also started to combine interests. My focus within computer science is game development, and I soon became interested in the ways that the aesthetic qualities of games can illustrate philosophical concepts, and how video games fit within the context of fiction and art. 
        </p>

        <h2 class="section-header">What do you already know or suspect about AI or Ethics?</h2>

        <p>
            I have some limited knowledge of how machine learning works, and relatively little about ethics, but there are several ethical questions surrounding AI that I suspect I will enjoy discussing. 
        </p>

        <p>
            I'm particularly drawn to questions of who owns creative work that AI produces, because I feel that such questions make us confront the way we view our own capacity for creative thought. Setting aside legal issues of copyright, how fundamentally different is an algorithm's creative process from our own? Do our brains not simply sift through information and combine it in ways we call “creative”? After all, Picasso himself said “Good artists copy, great artists steal.” And if it is the case that our brains are not all that different from machines, then beyond the current practical limits of our technology, to what extent could a machine theoretically… have rights?
        </p>

        <h2 class="section-header">What have you learned already in the first class, especially anything that challenged you or surprised you?</h2>

        <p>
            This first week of class, I was able to reflect on the pros and cons of the ethical theories we covered. I learned a lot more about the rationale behind virtue ethics as described in Driver's book (particularly the role that “practical wisdom” plays), and I also familiarized myself with Hume's version of virtue ethics.        </p>

        <p>
            Overall, when we were evaluating ethical theories, I noticed that there seems to be a general conviction that the majority of people are morally good. When we find that perfectionist moral virtues are unattainable, or that classical Utilitarianism is unrealistically demanding when taken to its logical extreme, it feels like a flaw in the theory. It's a natural conviction to have, but I'm curious about ways that particularly a Utilitarian might respond by simply accepting that everyone is evil to varying degrees, and the potential flaws this position might have. (I've read a little bit about Peter Singer before and I believe this is along the lines of what he argues, but I'm interested to learn more).
        </p>
    </div>
    <script>
        particlesJS("particles-js", {"particles":{"number":{"value":100,"density":{"enable":true,"value_area":800}},"color":{"value":"#222222"},"shape":{"type":"circle","stroke":{"width":0,"color":"#000000"},"polygon":{"nb_sides":5},"image":{"src":"img/github.svg","width":100,"height":100}},"opacity":{"value":0.5,"random":false,"anim":{"enable":false,"speed":1,"opacity_min":0.1,"sync":false}},"size":{"value":3,"random":true,"anim":{"enable":false,"speed":40,"size_min":0.1,"sync":false}},"line_linked":{"enable":true,"distance":200,"color":"#ffffff","opacity":0.2,"width":1},"move":{"enable":true,"speed":1,"direction":"none","random":false,"straight":false,"out_mode":"out","bounce":false,"attract":{"enable":false,"rotateX":600,"rotateY":1200}}},"interactivity":{"detect_on":"canvas","events":{"onhover":{"enable":true,"mode":"repulse"},"onclick":{"enable":false,"mode":"push"},"resize":true},"modes":{"grab":{"distance":400,"line_linked":{"opacity":1}},"bubble":{"distance":400,"size":40,"duration":2,"opacity":8,"speed":3},"repulse":{"distance":200,"duration":0.4},"push":{"particles_nb":4},"remove":{"particles_nb":2}}},"retina_detect":true});var count_particles, stats, update; stats = new Stats; stats.setMode(0); stats.domElement.style.position = 'absolute'; stats.domElement.style.left = '0px'; stats.domElement.style.top = '0px'; document.body.appendChild(stats.domElement); count_particles = document.querySelector('.js-count-particles'); update = function() { stats.begin(); stats.end(); if (window.pJSDom[0].pJS.particles && window.pJSDom[0].pJS.particles.array) { count_particles.innerText = window.pJSDom[0].pJS.particles.array.length; } requestAnimationFrame(update); }; requestAnimationFrame(update);;
    </script>
  </body>
</html>
