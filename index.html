<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AI Ethics Blog</title>
     
    <link rel="stylesheet" href="styles.css">
</head>
  <body>
    <header>
        <p class="header-text">Giancarlo Massimo - Intro to AI Ethics Class Blog - PHL 304T</p>
    </header>

    <div class="container">
        <h1>Introduction to AI Ethics</h1>
        <h3>August 25, 2023</h3>
        
        <hr>
        <h2 class="section-header">What interests you?</h2>

        <p>
            Though I'm a computer science major, I developed a deep interest in philosophy throughout highschool. I liked the process of unpacking layers of complexity in everyday things, and I knew, that whether conscious of it or not, my view of the world is influenced by countless hidden assumptions. 
        </p>

        <p>
            I saw philosophy as a way to challenge those assumptions. It was immediately obvious that it's basically impossible to reach any satisfying conclusions, but maybe over time I could learn to ask better questions and even gain some small measure of control over them. 
        </p>

        <p>
            I also started to combine interests. My focus within computer science is game development, and I soon became interested in the ways that the aesthetic qualities of games can illustrate philosophical concepts, and how video games fit within the context of fiction and art. 
        </p>

        <h2 class="section-header">What do you already know or suspect about AI or Ethics?</h2>

        <p>
            I have some limited knowledge of how machine learning works, and relatively little about ethics, but there are several ethical questions surrounding AI that I suspect I will enjoy discussing. 
        </p>

        <p>
            I'm particularly drawn to questions of who owns creative work that AI produces, because I feel that such questions make us confront the way we view our own capacity for creative thought. Setting aside legal issues of copyright, how fundamentally different is an algorithm's creative process from our own? Do our brains not simply sift through information and combine it in ways we call “creative”? After all, Picasso himself said “Good artists copy, great artists steal.” And if it is the case that our brains are not all that different from machines, then beyond the current practical limits of our technology, to what extent could a machine theoretically… have rights?
        </p>

        <h2 class="section-header">What have you learned already in the first class, especially anything that challenged you or surprised you?</h2>

        <p>
            This first week of class, I was able to reflect on the pros and cons of the ethical theories we covered. I learned a lot more about the rationale behind virtue ethics as described in Driver's book (particularly the role that “practical wisdom” plays), and I also familiarized myself with Hume's version of virtue ethics.        </p>

        <p>
            Overall, when we were evaluating ethical theories, I noticed that there seems to be a general conviction that the majority of people are morally good. When we find that perfectionist moral virtues are unattainable, or that classical Utilitarianism is unrealistically demanding when taken to its logical extreme, it feels like a flaw in the theory. It's a natural conviction to have, but I'm curious about ways that particularly a Utilitarian might respond by simply accepting that everyone is evil to varying degrees, and the potential flaws this position might have. (I've read a little bit about Peter Singer before and I believe this is along the lines of what he argues, but I'm interested to learn more).
        </p>

        <h1>Week 2 Blog Post</h1>
        <h3>September 1, 2023</h3>
        
        <hr>

        <p>
            This week we were asked to respond to "I Shouldn’t Have to Accept Being in Deepfake Porn" by Nina Jankowicz in The Atlantic with either a utilitarian or virtue ethicist evaluation. I’ll be giving a utilitarian evaluation of the article because I think it highlights the ways that oversimplified formulations of utilitarianism can lead to obviously terrible conclusions, and the ways that we might look to revise utilitarianism while keeping its core principles. 
        </p>

        <p>
            This case is not ethically challenging in the sense that we wonder what the conclusion should be—no remotely moral person could ever justify the creation of deepfake porn. Rather, it’s challenging in the sense that it makes us rethink our general moral principles and adjust them to be more robust. This is an especially important goal considering that with the rapid pace of technological advancement, new ethical challenges are always arising. 
        </p>

        <p>
            We can start with a basic form of utilitarianism: an action is considered ethical if it increases pleasure and minimizes pain for the greatest number of people. While this principle seems intuitive at first, we can quickly see how it leads to extremely terrible conclusions. In the case of deepfake porn, Jankowicz writes: “By simply existing as women in public life, we have all become targets, stripped of our accomplishments, our intellect, and our activism and reduced to sex objects for the pleasure of millions of anonymous eyes.” The most basic form of utilitarianism would seem to suggest that the pleasure of millions outweighs that of one, even if it means that one person’s fundamental rights are atrociously disregarded. 
        </p>

        <p>
           This case may give us reason to give up utilitarianism entirely and adopt more of an “Ethics of Care” approach. Perhaps there are no general principles we can universally appeal to, and each case needs to be evaluated in the context of empathy. Deepfake porn is categorically wrong simply because we have empathy for each other as human beings.  
        </p>

        <p>
            However, I do think there are ways that the utilitarian view can adapt. I’ll bring in an example we went over in class. Suppose we are all stranded on an island. We collectively decide to be a utilitarian society and to divide up the island into plots of land for each of us. Now suppose that we want to build a swimming pool in the middle of the island, taking away someone’s plot of land. The result is a loss of pleasure for the person whose plot was taken, but an increase in pleasure for everyone else, so basic act-utilitarianism would counterintuitively conclude that this is morally acceptable. 
        </p>

        <p>
            Nevertheless, we can introduce rule-utilitarianism, and say that we should increase overall utility, but not at the expense of anyone’s fundamental rights. In this case, we would conclude that we cannot build a swimming pool. On the other hand, we can modify the example and instead say that there is an important medicine buried underneath one person’s plot of land, and we need it to save everyone else on the island. In this case, the need to save everyone would seem to outweigh the rule that we shouldn’t take anyone’s plot of land, so rule-utilitarians are still willing to break the rules if necessary. The question then becomes: when is it okay for us to break the rules? It seems that the case of the swimming pool and medicine are identical, except at different extremes—in both cases we are trading a decrease in utility for one person for an increase in utility of many others. Where should the line be?
        </p>

        <p>
            This is where I believe the rule-utilitarian can respond and say that the two cases are not the same at all. It’s true that we trade a decrease in one person’s utility for an increase in the utility of many others, but there ought to be a morally significant difference between increasing pleasure and alleviating suffering. In the swimming pool example, everyone may be doing completely fine and would simply enjoy having a pool, but in the medicine example there is risk of great suffering if we don’t break the rule. 
        </p>

        <p>
            Finally, we can see how this applies to the case of deepfake porn. If we agree that people ought to have fundamental rights to bodily autonomy, privacy, consent, etc., then under a rule-utilitarian view, it is never acceptable to violate these rights, no matter how many people may wrongfully enjoy it. Only at the risk of many people suffering greatly could we consider breaking the rules, and there is obviously no such risk here. We can even bring in the views of John Sturart Mill and argue that on top of that, there are higher and lower pleasures, the lower pleasures being the ones we share with animals. So even if people do gain pleasure, those pleasures are not worth much to begin with, certainly not enough to commit an egregious violation of a person’s basic rights. 
        </p>
    </div>
  </body>
</html>
